# Python爬虫基础教程

## 1. 什么是爬虫？

爬虫（Web Crawler）是一种自动化程序，用于模拟人类浏览网页，从互联网上批量获取数据。它可以帮助我们自动化地收集、整理和分析网络上的信息，广泛应用于数据分析、信息检索、舆情监控等领域。

## 2. 基础知识准备

- **Python基础语法**（变量、循环、函数、异常处理等）
- **HTML基础**（了解标签结构、DOM树、常见标签如`<a>`、`<div>`、`<span>`等）
- **常用库**：`requests`（发送HTTP请求）、`BeautifulSoup`（解析HTML）、`lxml`（高效解析）、`re`（正则表达式）

## 3. 安装必备库

在命令行中运行以下命令安装所需库：

```bash
pip install requests beautifulsoup4 lxml
```

## 4. 第一个爬虫示例

### 4.1 获取网页内容

```python
import requests

url = 'https://www.example.com'
response = requests.get(url)
print(response.text)  # 输出网页HTML内容
```

> **提示：** 有些网站需要添加请求头（如User-Agent）才能正常访问。

### 4.2 解析网页内容

```python
from bs4 import BeautifulSoup

html = response.text
soup = BeautifulSoup(html, 'html.parser')
print(soup.title.text)  # 输出网页标题
```

### 4.3 提取指定内容

例如，提取页面中的所有链接：

```python
for link in soup.find_all('a'):
    print(link.get('href'))
```

你还可以提取图片、表格、段落等内容：

```python
# 提取所有图片链接
for img in soup.find_all('img'):
    print(img.get('src'))
```

## 5. 常见问题与注意事项

- **反爬虫机制**：部分网站会限制频繁访问。可以通过添加请求头、设置延时、使用代理或IP池等方式应对。
- **编码问题**：有时需要手动指定编码，如`response.encoding = 'utf-8'`。
- **数据存储**：可将数据保存为`csv`、`json`、数据库等格式，便于后续分析和处理。

例如，简单保存电影标题到CSV的示例：

```python
import requests
from bs4 import BeautifulSoup
import csv
import time

with open('movies.csv', 'w', newline='', encoding='utf-8') as f:
    writer = csv.writer(f)
    writer.writerow(['电影标题'])
    for page in range(0, 250, 25):
        url = f'https://movie.douban.com/top250?start={page}'
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'}
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, 'html.parser')
        movies = soup.find_all('div', class_='item')
        for movie in movies:
            title_tag = movie.find('span', class_='title')
            if title_tag:
                writer.writerow([title_tag.get_text(strip=True)])
        time.sleep(1)  # 增加延时，避免被封禁
```

- 遵守网站的`robots.txt`协议（声明网站允许或禁止爬虫抓取的内容，建议先查看并遵守），不要恶意爬取。可参考：[什么是robots.txt？](https://developers.google.com/search/docs/crawling-indexing/robots/intro?hl=zh-cn)
- 控制访问频率，避免对网站造成压力。
- 仅用于学习和研究，勿用于商业用途。
- 可进一步学习`xpath`、`selenium`等更高级的爬虫工具，以及数据清洗与分析方法。
- 注意处理异常和错误（如网络超时、页面不存在等），提高爬虫的健壮性。

## 6. 进阶内容推荐

- **XPath选择器**：比CSS选择器更强大，适合复杂页面结构解析。
- **Selenium自动化**：可模拟浏览器操作，处理JavaScript渲染页面。
- **多线程/异步爬虫**：提升爬取效率，如`threading`、`asyncio`、`aiohttp`等。
- **数据清洗与分析**：结合`pandas`、`numpy`等库进行数据处理。

## 7. 常见反爬虫手段及应对

- **验证码**：可尝试手动输入、打码平台或OCR识别。
- **登录验证**：使用会话（`requests.Session`）或模拟登录。
- **IP封禁**：使用代理池、动态IP。
- **动态加载数据**：分析接口，或用Selenium模拟操作。

---

**推荐学习资源：**

- [菜鸟教程 - Python爬虫](https://www.runoob.com/python3/python3-webbug.html)
- [廖雪峰Python教程](https://www.liaoxuefeng.com/wiki/1016959663602400/1017625380741664)
- [Python官方文档](https://docs.python.org/zh-cn/3/)
- [BeautifulSoup官方文档](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/)

祝你学习愉快，早日掌握爬虫技术！
